{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02/14/2018\n",
    "#Classify EDI Incidents, within Visibility, into Tier\n",
    "\n",
    "#load data from db (file)\n",
    "#data prep, EDA\n",
    "#fit model\n",
    "#predict (future!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of libraries\n",
    "\n",
    "import sys #used in error handling\n",
    "import time #to track performance time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt #to plot data\n",
    "\n",
    "#part1 - build model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer #to tokenize dataset, aka bag of words activity\n",
    "from sklearn.feature_extraction import text #to use stop_words\n",
    "from sklearn.naive_bayes import MultinomialNB #to run Naive Bayes algorithm for text classification\n",
    "from sklearn.pipeline import Pipeline #streamline Tokenizing + Classification model\n",
    "from sklearn import metrics # to evaluate model\n",
    "\n",
    "#part2 - grid search\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.externals import joblib #to pickle the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbose mode: Y or N? y\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------- set CONSTANTS and variables -----------------------------------------------\n",
    "#prompt to select verbose mode to view progress details or not\n",
    "while True:\n",
    "    inp = input(\"Verbose mode: Y or N? \")\n",
    "    if inp.lower() not in ('y','n'):\n",
    "         print(\"Please enter valid verbose mode: Y or N? \")\n",
    "    else:\n",
    "        fn_verbose = lambda x: 0 if x=='N' else 1\n",
    "        input_verbose_flg=fn_verbose(inp.upper())\n",
    "        break\n",
    "\n",
    "#----------------------------------------------- DEFINE GLOBAL FUNCTIONS -----------------------------------------------\n",
    "### define functions to classify incidents\n",
    "\n",
    "def fn_classify(x):\n",
    "    if x !='Tier 1' and x !='Tier 2' and x !='Tier 3':\n",
    "        return 'not'\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "#reclassify Y output to numeric for SGD (lambda function)\n",
    "#output aka categories -> data.target equivalent\n",
    "fn_classify_numeric = lambda x: 0 if x=='Tier 1' else 1 if x=='Tier 2' else 2 if x=='Tier 3' else 3\n",
    "\n",
    "#reclassify Y numeric output to text from SGD (lambda function)\n",
    "categories=np.array(['Tier 1','Tier 2','Tier 3','not'])\n",
    "fn_classify_text = lambda x: categories[x] #doen's work with -1,0,1,2 list.  not becomes 0 which is incorrect\n",
    "#fn_classify_text = lambda x: \"not\" if x==-1 else 'Billing' if x==0 else 'OrderMgmt' if x==1 else 'Transportation'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDI Incidents Queue Classification\n",
    "\n",
    "### Abstract\n",
    "This code demonstrates how to build a prediciton model for multiclass text classification.  The goal is to predict queue category (Billing, Order Management, Transportation, and not) given the text contents of the submitted EDI incidents.  \n",
    "\n",
    "Key libraries:  \n",
    "a Naive Bayes algorithm from Scikit-learn library. \n",
    "\n",
    "\n",
    "### 0. Load Data\n",
    "Load data from Service Desk database\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step0: Loading data from file...\n",
      "All data: (20, 3) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 3: expected 4 fields, saw 5\\nSkipping line 18: expected 4 fields, saw 5\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>à¥ÇDO97*~§ÈÉ¸8ÀOíc\u001c",
       "|n¦Ñ\u0007ä\u0004Eøÿ\u0014ö\u0011éºóÀB\u0010ÉÀ!$}íàÈé;{ìÐå[îñé2þ\u0006</th>\n",
       "      <th>\u00148æø(%\u0019£¦\"Dô¹Ò4jÎ0u2jsÐ\u001d",
       "ÊMYÞË´äúSì­´·· )fåÿ¹CÛö\u0006y\u001fÑó\u0019\tI&lt;\r",
       "y</th>\n",
       "      <th>ËôïfäÉÇÕß</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PK\u0003\u0004\u0014</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>*&amp;$Ú#Äú-\u0018j½\u0016\u0019_¨¬©¸´{\u0010ÍKf¾)Dm:´*{\u000b",
       "\\Åô¦©ýLU5@¬D)ì}\u000b",
       "¼*Ïn¥ÒlUBØ;Ò÷v\u001a®\bþ\u0004Ã\u0010vÀôÊU%2­ZÛS\u000eö¤_ÅOp@È³\u0014ì^çàmH4ÐüN¸\u001a\u001e",
       "Yéè/YEG¬è\u0011àF# ­V+1$ï/ÑúGn!×¢äß÷ÒõX]e«T¼\u0019»Ì",
       "åy\u0006°U[þìnêY#J°\u0012ÜÃ}\u0014Lr¾Ð^Î×¬)í\u0015\b¹\u0007£h\u0014¶O0¦¥åZ2ËçJZÐá!®ÕÜd\f",
       "ØóBÂ½oüg#4Æ\u0002}A¬0²</th>\n",
       "      <td>f+sÁlá5ºLPpm xè8\u0005­ÇQ2Xp³±ª\u000e¨½n",
       "?Ð'Ë\\z#³ýú...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ã\u000eû\u001e",
       "]Ä6KrèÒ¹\u00183=vØP&lt;sÚLÈÄ~&amp;\u000e!EwI\u0017|Ù;D=C\u001c",
       "P²1Ü·\\t¶Âýf\"¸\u0005äj'úeÎ\u001d",
       "±¼½\u001f\u0017t°eÚ&lt;¶ØµÍ3;:ó©Ú»\u0018StÆ\u0018{·&gt;sXÐa3Ëç¹ÑW\"`\u001d",
       "ìJ¬+ÈÎUõ`\u0001eªkÖ)r\b+e÷ñm°goqx\u0016(\u0011ß¤ù\u001aDÝJ]8åTz\u000eMà5\u0002å\u001fäÓ)×\u0005è0»¿Ië\bYgz\u0016î|]p+~o³Ç`_Þ=í¾\u0004\u0019|j\u0019 ö·öÍ\u0010Qk&lt;a\b</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\u0017Ý\u0015þ\\D«ZlîØ6\u000f\u0003\u0014FV½\u0013äÅÏ²'üwÊ\u001e",
       "w\u0001s\u0006\u0005[ñû:(eçD³\\t÷</th>\n",
       "      <td>kzhÜÀp¬sÖyUs^ÕøÿûªfÓ^&gt;¯eÎkóZÆõöõAj¼|Ê&amp;ïòè...</td>\n",
       "      <td>Ü#-ãq&amp;?'2ÚÐ\f",
       "ZCeÝÀLõTx3&amp; c¤u+\u0015Ð­ûNóxÓNg...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0m\u0015\u0015xåöàE½åAÚAf\u001c",
       "çc\u0015§´¼®</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ÎFz3©\u0001Pb/3 tSÙºqyjuiª½E¤-#t³0Ò0\u0017á</th>\n",
       "      <td>;ÍûYÆºÔ2O¹b¹\u001br3ê\u000f\u0011kE\"'¸&amp;&amp;SÐÄ;nùµj\b·*#4kù\u0013...</td>\n",
       "      <td>\b\f",
       "OÔ6ømuF8=ñ®'?ÈÌZu@</td>\n",
       "      <td>ëJøúÊÜ¼Õf\u0007w&lt;zp8§RèPBo#(úÒ\u001bÈ6`ÜY\bß¼9'-ÿ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   à¥ÇDO97*~§ÈÉ¸8ÀOíc\n",
       "|n¦Ñ\u0007ä\u0004Eøÿ\u0014ö\u0011éºóÀB\u0010ÉÀ!$}íàÈé;{ìÐå[îñé2þ\u0006  \\\n",
       "PK\u0003\u0004\u0014                                                                                                                  \n",
       "*&$Ú#Äú-\u0018j½\u0016\u0019_¨¬©¸´{\u0010ÍKf¾)Dm:´*{\n",
       "\\Åô¦©ýLU5@¬D)...  f+sÁlá5ºLPpm xè8\u0005­ÇQ2Xp³±ª\u000e¨½n\n",
       "?Ð'Ë\\z#³ýú...                  \n",
       "\n",
       "ã\u000eû\n",
       "]Ä6KrèÒ¹\u00183=vØP<sÚLÈÄ~&\u000e!EwI\u0017|Ù;D=C...                                                NaN                  \n",
       "\n",
       "\u0017Ý\u0015þ\\D«ZlîØ6\u000f\u0003\u0014FV½\u0013äÅÏ²'üwÊ\n",
       "w\u0001s\u0006\u0005[ñû...  kzhÜÀp¬sÖyUs^ÕøÿûªfÓ^>¯eÎkóZÆõöõAj¼|Ê&ïòè...                  \n",
       "0m\u0015\u0015xåöàE½åAÚAf\n",
       "çc\u0015§´¼®                                                                      NaN                  \n",
       "ÎFz3©\u0001Pb/3 tSÙºqyjuiª½E¤-#t³0Ò0\u0017á            ;ÍûYÆºÔ2O¹b¹\u001br3ê\u000f\u0011kE\"'¸&&SÐÄ;nùµj\b·*#4kù\u0013...                  \n",
       "\n",
       "                                                   \u00148æø(%\u0019£¦\"Dô¹Ò4jÎ0u2jsÐ\n",
       "ÊMYÞË´äúSì­´·· )fåÿ¹CÛö\u0006y\u001fÑó\u0019\\tI<\\ry  \\\n",
       "PK\u0003\u0004\u0014                                                                                                                      \n",
       "*&$Ú#Äú-\u0018j½\u0016\u0019_¨¬©¸´{\u0010ÍKf¾)Dm:´*{\n",
       "\\Åô¦©ýLU5@¬D)...                                                NaN                      \n",
       "\n",
       "ã\u000eû\n",
       "]Ä6KrèÒ¹\u00183=vØP<sÚLÈÄ~&\u000e!EwI\u0017|Ù;D=C...                                                NaN                      \n",
       "\n",
       "\u0017Ý\u0015þ\\D«ZlîØ6\u000f\u0003\u0014FV½\u0013äÅÏ²'üwÊ\n",
       "w\u0001s\u0006\u0005[ñû...  Ü#-ãq&?'2ÚÐ\n",
       "ZCeÝÀLõTx3& c¤u+\u0015Ð­ûNóxÓNg...                      \n",
       "0m\u0015\u0015xåöàE½åAÚAf\n",
       "çc\u0015§´¼®                                                                      NaN                      \n",
       "ÎFz3©\u0001Pb/3 tSÙºqyjuiª½E¤-#t³0Ò0\u0017á                                        \b\n",
       "OÔ6ømuF8=ñ®'?ÈÌZu@                      \n",
       "\n",
       "                                                                                            ËôïfäÉÇÕß  \n",
       "PK\u0003\u0004\u0014                                                                                                  \n",
       "*&$Ú#Äú-\u0018j½\u0016\u0019_¨¬©¸´{\u0010ÍKf¾)Dm:´*{\n",
       "\\Åô¦©ýLU5@¬D)...                                                NaN  \n",
       "\n",
       "ã\u000eû\n",
       "]Ä6KrèÒ¹\u00183=vØP<sÚLÈÄ~&\u000e!EwI\u0017|Ù;D=C...                                                NaN  \n",
       "\n",
       "\u0017Ý\u0015þ\\D«ZlîØ6\u000f\u0003\u0014FV½\u0013äÅÏ²'üwÊ\n",
       "w\u0001s\u0006\u0005[ñû...                                                NaN  \n",
       "0m\u0015\u0015xåöàE½åAÚAf\n",
       "çc\u0015§´¼®                                                                      NaN  \n",
       "ÎFz3©\u0001Pb/3 tSÙºqyjuiª½E¤-#t³0Ò0\u0017á            ëJøúÊÜ¼Õf\u0007w<zp8§RèPBo#(úÒ\u001bÈ6`ÜY\bß¼9'-ÿ...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------------- Step 0. Load Data ------------------------------------------------------------------\n",
    "# Load data directly from Service Desk database into pandas dataframe\n",
    "# this query includes an index column, some categorical columns, some free text columns + target column\n",
    "# -------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nStep0: Loading data from file...\")\n",
    "\n",
    "#track performance time\n",
    "t0=time.time()\n",
    "\n",
    "data_all=pd.read_csv('dummy.xlsx',index_col=0, delimiter=\",\", encoding = \"ISO-8859-1\" , error_bad_lines=False,\n",
    "                 lineterminator='\\n')\n",
    "\n",
    "print (\"All data: {} \".format(data_all.shape))\n",
    "data_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Prep\n",
    "Build a learning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step1: Creating a tidy dataset...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nStep1: Creating a tidy dataset...\")\n",
    "\n",
    "# ------------------------Step 1. Data Prep-------------------------------------------------------------------------------------------\n",
    "# prepare a tidy dataset\n",
    "# -------------------------------------------------------------------------------------------------------------------------------\n",
    "#Ready for tidy dataset\n",
    "df_tidy=data_all.copy()\n",
    "df_tidy.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found duplicates: 11 \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot index with vector containing NA / NaN values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-c8c3058b13b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_tidy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mduplicates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf_tidy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mduplicates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#after duplicate analysis, decide what to do with them: keep or drop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1580\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mis_bool_indexer\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                     raise ValueError('cannot index with vector containing '\n\u001b[0m\u001b[1;32m    192\u001b[0m                                      'NA / NaN values')\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot index with vector containing NA / NaN values"
     ]
    }
   ],
   "source": [
    "#check for duplicate records: varies by project \n",
    "print (\"Found duplicates: {:d} \".format(sum(df_tidy.duplicated(keep=False))))\n",
    "\n",
    "#if duplicates found, examine them \n",
    "duplicates = df_tidy.duplicated(keep=False)\n",
    "df_tidy[duplicates].index\n",
    "\n",
    "df_tidy.loc[data_all[duplicates].index]\n",
    "\n",
    "#after duplicate analysis, decide what to do with them: keep or drop\n",
    "#df_tidy.drop_duplicates\n",
    "\n",
    "#No duplicates found here, based on unique index by Incident ID; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select relevant columns: varies by project\n",
    "#chose option 1: load many columns from db and filter out here\n",
    "print (\"Select relevant columns: \")\n",
    "\n",
    "#alternative syntax, select columns by number\n",
    "#tidy_include = [df[7:10]]\n",
    "tidy_include = ['Summary','Description','Urgency','ReasonForUrgency','AffectedService', 'Tier']\n",
    "\n",
    "df_tidy = df_tidy[tidy_include]\n",
    "print (df_tidy.shape)\n",
    "df_tidy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing values\n",
    "#i.e. records with neither summary nor descripton, i.e. 578445\n",
    "\n",
    "print (\"\\nCheck for missing values in Description:{:d} \".format(sum(df_tidy['Description'].isnull())))\n",
    "print (\"Check for missing values in Summary: {:d} \".format(sum(df_tidy['Summary'].isnull())))\n",
    "print (\"Check for missing values in both Description and Summary: {:d} \".format(sum(df_tidy['Summary'].isnull() & df_tidy['Summary'].isnull()))) \n",
    "\n",
    "#filter out records with missing data\n",
    "if sum(df_tidy['Description'].isnull() & df_tidy['Summary'].isnull()):\n",
    "    df_tidy=df_tidy[df_tidy['Description'].notnull() | df_tidy['Summary'].notnull()]\n",
    "    print (\"Filtered data -> removed records with missing data:{} \".format(df_tidy.shape))\n",
    "    \n",
    "#plot distribution of categories\n",
    "df_tidy['Tier'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Perform feature engineering: concatentate text columns\")\n",
    "#after checking for missing data, concatenate relevant fields (Description, Summary, etc) into one text field\n",
    "\n",
    "#doesn't work when no 'Description'\n",
    "#data['all_text'] = data['Summary'].str.cat(data['Description'], sep='__HERE__')\n",
    "\n",
    "df_tidy['all_text'] = df_tidy['Summary'].fillna('') + df_tidy['Description'].fillna('') + df_tidy['Urgency'].fillna('')+ df_tidy['ReasonForUrgency'].fillna('')\n",
    "    \n",
    "print (\"Total records after concatenation:{} \".format(df_tidy.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependent Variable, aka Target\n",
    "Choose a target variable and format it to serve as a classifying \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure target variable is categorical\n",
    "df_tidy['Tier']=df_tidy['Tier'].astype('category')\n",
    "df_tidy.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine target variable\n",
    "\n",
    "#how many categories total?\n",
    "print (\"Number of categories: {:d} \".format(len(df_tidy['Tier'].cat.categories)))\n",
    "print (df_tidy['Tier'].cat.categories) \n",
    "#alternative way to count categories\n",
    "#df_tidy_grpByCategory = df_tidy.groupby(['AffectedService']).size()\n",
    "#print \"Number of categories: \", df_tidy_grpByCategory.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Data Prep Summary\n",
    "\n",
    "\n",
    "decide to include some columns but not others\n",
    "determine what to do with missing values, i.e. nulls and NAs\n",
    "perform feature engineering, i.e. combine all text fields into one\n",
    "define final class \"labels\", i.e. focus on most occuring categories\n",
    "\n",
    "Tidy data ->\n",
    "- verified no duplicates\n",
    "- selected relevant columns: 'Summary','Description','Urgency','ReasonForUrgency','AffectedService'\n",
    "- dropped records with missing data\n",
    "- performed feature engineering: concatenated text fields into 'all_text' column\n",
    "- defined a target variable: 'AffectedService' column \n",
    "\n",
    "Ready for next step: where tidy data will be transformed into format ready for ML: X (matrix) and y (vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at all incidents by category  \n",
    "print (\"Total EDI incidents: {}\".format(df_tidy.shape[0]))\n",
    "print (\"Number of categories: {:d}\".format(len(df_tidy['Tier'].cat.categories)))\n",
    "print (\"Categories: {}\".format(df_tidy['Tier'].cat.categories))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ready to proceed to ML!\n",
    "\n",
    "### 1c. Prep Data for ML\n",
    "Assemble Data into ML Expected Format.  Scikit-learn expects a Numpy array-like structure. Transform the tidy dataset to a structure acceptable by algorithm: \n",
    "- input features X(matrix) and \n",
    "- target variable y(vector). \n",
    "\n",
    "X - column 'all_text'  \n",
    "y - column 'AffectedService', reformatted with values  'Billing', 'OrderMgmt', 'Transportation', and 'not'.\n",
    "\n",
    "Split data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------- Step 1c. Prep Data for ML ----------------------S----------------------------------------\n",
    "print (\"\\nStep1c: Prepping data for ML...\")\n",
    "start = time.time()\n",
    "\n",
    "columns_selected=['all_text','Tier']\n",
    "data=df_tidy[columns_selected]\n",
    "\n",
    "#rename columns to fit into ML text classification\n",
    "data.columns = ['text', 'class']\n",
    "data['class_multi']=data['class'].apply(fn_classify)\n",
    "print (\"Filtered data: {}\".format(data.shape))\n",
    "\n",
    "#create data.data and data.target equivalents without converting to a list\n",
    "X=data.text\n",
    "y=data.class_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train/Test Split\n",
    "# Randomly split data into two groups: a training set and a validation set\n",
    "# It's important not to touch the test set when building a classifier. \n",
    "#Therefore, we separate X&y into two sets: for training the model and for testing the model accuracy.  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30)\n",
    "\n",
    "#done - print summary \n",
    "#sptodo: print \"Done in %0.3fs\" % (end - start)    \n",
    "if input_verbose_flg ==1:\n",
    "    print (\"\\nTotal records and fields in learning set: {}\".format(data.shape[0]))\n",
    "    print (\"Total files in training set: {}\".format(len(X_train)))\n",
    "    print (\"Total files in test set: {}\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for all dataframes\n",
    "alldfs = [var for var in dir() if isinstance(eval(var), pd.core.frame.DataFrame)]\n",
    "print(alldfs) # df1, df2\n",
    "\n",
    "\n",
    "# RELEASE MEMORY\n",
    "lst = [df,df_tidy]\n",
    "del df\n",
    "del df_tidy\n",
    "del lst   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Classifying Incidents - Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- Step 2. Classifying Incidents - Train Model -----------------------------------------------\n",
    "# algorithm: MultinomialNB or SGDClassifier\n",
    "# do grid search to find best hyper-parameters (based on SGD example) \n",
    "# note: MultinomialNB takes categories as is, no need to convert to int)\n",
    "# note: SGDClassifier can't process categories as is, an extra setp to to convert to int)\n",
    "\n",
    "print (\"\\nParameters for training a classifier model...\")\n",
    "\n",
    "# prompt to select a classifier\n",
    "while True:\n",
    "    inp = input(\"Select learning algorithm: (NB)-Multinomial Naive Bayes or (SGD)-Stochastic Gradient Descent: \")\n",
    "    if inp.lower() not in ('nb','sgd'):\n",
    "         print (\"Please select a valid algorithm: (NB)-Multinomial Naive Bayes or (SGD)-Stochastic Gradient Descent \")\n",
    "    else:\n",
    "        input_algorithm=inp.upper()\n",
    "        break\n",
    "        \n",
    "# prompt to select unigrams or +bigrams\n",
    "while True:\n",
    "    try:\n",
    "        input_ngram = int(input(\"Select (1)-unigrams only or (2)-unigrams and bigrams: \")) \n",
    "    except ValueError:\n",
    "        print (\"Sorry I didn't understand that\")\n",
    "        continue\n",
    "    else:\n",
    "        #file size successfully parsed\n",
    "        if input_ngram < 1 or input_ngram > 2:\n",
    "            print (\"Please select a valid n-gram range: (1)-unigrams only or (2)-unigrams and bigrams\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "# prepare hyper-parameters for grid search\n",
    "# Our classifier has a few hyper-parameters. The two most important are:\n",
    "\n",
    "# The alpha keyword in the Bayesian classifier is a \"smoothing parameter\" -- increasing the value decreases the sensitivity to any single feature, and tends to pull prediction probabilities closer to 50%.\n",
    "alphas = (.001, 0.0001,0.00001) #(.001, .01, .1, 1) #sptodo try smaller (0.00001, 0.000001) #was  (.001, .01, .1, 1, 5, 10)\n",
    "# The min_df keyword in CountVectorizer, which will ignore words which appear in fewer than min_df fraction of reviews. Words that appear only once or twice can lead to overfitting, since words which occur only a few times might correlate very well with document classes by chance in the training dataset.\n",
    "min_dfs = (1e-5, 1e-4)\n",
    "max_dfs = (0.7, 0.9)\n",
    "# ngrams - unigrams only or unigrams and bigrams\n",
    "if input_ngram==1:\n",
    "    ngram_range=[(1,1)]\n",
    "elif input_ngram==2:\n",
    "    ngram_range=[(1, 1), (1, 2)]\n",
    "else:\n",
    "    ngram_range=-1 \n",
    "    \n",
    "#if want to use stop_words\n",
    "#type(text.ENGLISH_STOP_WORDS)\n",
    "my_additional_stop_words=['hi','hello','dear','helpdesk']\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)\n",
    "#print (my_stop_words)\n",
    "            \n",
    "# prepare pipeline for grid search\n",
    "if input_algorithm =='NB':\n",
    "    #the grid of parameters to search over\n",
    "    parameters = {\n",
    "        'vect__min_df': min_dfs,\n",
    "        'vect__max_df': max_dfs,\n",
    "        'vect__ngram_range': ngram_range,\n",
    "        #'tfidf__use_idf': (True, False),\n",
    "        'clf__alpha': alphas\n",
    "    }\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(stop_words=my_stop_words)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf',  MultinomialNB()),\n",
    "    ])\n",
    "elif input_algorithm=='SGD':\n",
    "    #the grid of parameters to search over\n",
    "    parameters = {\n",
    "        'vect__min_df': min_dfs,\n",
    "        'vect__max_df': max_dfs,\n",
    "        'vect__ngram_range': ngram_range,\n",
    "        #'tfidf__use_idf': (True, False),\n",
    "        'clf__penalty': ('l2', 'elasticnet'),\n",
    "        'clf__alpha': alphas\n",
    "        }\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(stop_words=my_stop_words)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', SGDClassifier()),\n",
    "    ])\n",
    "    \n",
    "    #reclassify Y output to numeric for SGD\n",
    "    y_train_backup=y_train\n",
    "    y_test_backup=y_test #save and reassign at the end, so this section can be run again without erroring out\n",
    "    y_train=np.array(y_train.apply(fn_classify_numeric))\n",
    "    y_test=np.array(y_test.apply(fn_classify_numeric))\n",
    "\n",
    "# prepare grid search - find the best parameters for both the feature extraction and the classifier\n",
    "# n_jobs=-1 grid search will detect how many cores are installed and uses them all\n",
    "# cv defaults to 3 folds\n",
    "classifier_grid = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=input_verbose_flg) \n",
    "if input_algorithm =='NB':\n",
    "    print (\"\\nStep2: Training a Naive Bayes model...\")\n",
    "elif input_algorithm =='SGD':\n",
    "    print (\"\\nStep2: Training a Stochastic Gradient Descent model...\")\n",
    "\n",
    "# ********************************     FIT MODEL  *****************************************************************************#\n",
    "t0 = time.time()\n",
    "print (\"Performing grid search... (this may take up to 10 minutes)\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\", parameters)\n",
    "\n",
    "classifier_grid.fit(X_train, y_train)\n",
    "\n",
    "#done - print summary \n",
    "t1 = time.time()\n",
    "print (\"Done in %0.3fs\" % (t1 - t0))\n",
    "\n",
    "best_score = classifier_grid.best_score_\n",
    "best_parameters = classifier_grid.best_estimator_.get_params()\n",
    "\n",
    "print (\"Best score: %0.3f\" % best_score)\n",
    "print (\"Best parameters set:\")\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print (\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "print(\"\\nProcessing time to fit model (in min): \", (t1 - t0)/60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------- 3. Evaluate Model Performance -----------------------------------------------\n",
    "### 3a. Evaluate High Level Accuracy\n",
    "# run on a test set  \n",
    "# Report the accuracy of this model on both the training and testing data.  Are results comparable?\n",
    "\n",
    "print (\"\\nStep3: Evaluating model performance...\")\n",
    "\n",
    "y_hat=classifier_grid.predict(X_test)\n",
    "\n",
    "#done - print summary \n",
    "end=time.time()\n",
    "\n",
    "accuracy_train=classifier_grid.score(X_train, y_train)\n",
    "accuracy_test=classifier_grid.score(X_test, y_test)\n",
    "\n",
    "print (\"Done in %0.3fs\" % (end - start))\n",
    "print (\"\\nAccuracy on trainset: %0.4f \" % accuracy_train)\n",
    "print (\"Accuracy on cv set: %0.4f (aka best score from grid search)\" % best_score )\n",
    "print (\"Accuracy on testset: %0.4f \" %  accuracy_test)\n",
    "\n",
    "### 3c. More Analysis on the Model Accuracy:\n",
    "if input_verbose_flg == 1:\n",
    "    print (\"Total files and fields in testset:\", len(X_test))\n",
    "    \n",
    "#confusion matrix\n",
    "#from sklearn import metrics\n",
    "    print (\"Classified categories: \", best_parameters.get('clf').classes_)\n",
    "    print (\"\\nConfusion Matrix\" )\n",
    "    print(metrics.confusion_matrix( y_test, y_hat))\n",
    "\n",
    "print (\"\\nClassification Report\")\n",
    "print (metrics.classification_report(y_test, y_hat))\n",
    "\n",
    "print (\"Model built.  Ready to predict.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(classifier_grid, \"models/model_Classify.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
